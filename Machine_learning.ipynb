{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## Import all the necessary files modules and routines\nimport tensorflow\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow import keras\nimport numpy as np\nimport pandas as pd\nimport pickle as pkl\ntensorflow.keras.backend.clear_session()\nfrom tensorflow.keras.layers import Concatenate\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D, UpSampling2D, ZeroPadding2D\nfrom keras.layers import Input, add\nfrom keras.models import Model\nfrom matplotlib import image\nfrom matplotlib import pyplot\nfrom ipywidgets import interact\nimport matplotlib.pyplot as plt\nfrom skimage.transform import resize\nfrom numpy import genfromtxt\nfrom tensorflow.keras.utils import to_categorical","metadata":{"id":"2xI8J4LyGFEd","outputId":"91df4321-82da-4f63-d523-91c4eed99731","execution":{"iopub.status.busy":"2022-10-08T16:25:09.060049Z","iopub.execute_input":"2022-10-08T16:25:09.060804Z","iopub.status.idle":"2022-10-08T16:25:09.070580Z","shell.execute_reply.started":"2022-10-08T16:25:09.060757Z","shell.execute_reply":"2022-10-08T16:25:09.069457Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"\n## Import Low Resolution images as np array and split in (32*32) format \n\nY=np.load('../input/dataset-new/Lowres.npy',allow_pickle=True)\n\nx_original=np.split(Y,945)\n\n\n# Plotting images for easy visualization of Structures\ndef plot_bridge(i):\n    fig,ax = pyplot.subplots()\n    im = pyplot.imshow(x_original[i], cmap='gray_r')\n    pyplot.axis('off')\n    \n    pyplot.show()\n\ninteract(plot_bridge, i=(0, len(x_original)-1))\n","metadata":{"id":"yBOGrWs3Gmen","outputId":"b757ff3f-80eb-4f3b-c2af-7adb16b2a752","execution":{"iopub.status.busy":"2022-10-08T16:25:11.014367Z","iopub.execute_input":"2022-10-08T16:25:11.014741Z","iopub.status.idle":"2022-10-08T16:25:11.255013Z","shell.execute_reply.started":"2022-10-08T16:25:11.014689Z","shell.execute_reply":"2022-10-08T16:25:11.253844Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"interactive(children=(IntSlider(value=472, description='i', max=944), Output()), _dom_classes=('widget-interacâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8de8c83414114304a9268820fff548a6"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"<function __main__.plot_bridge(i)>"},"metadata":{}}]},{"cell_type":"code","source":"X_input=np.array(x_original)","metadata":{"execution":{"iopub.status.busy":"2022-10-08T16:25:11.858050Z","iopub.execute_input":"2022-10-08T16:25:11.858404Z","iopub.status.idle":"2022-10-08T16:25:11.868807Z","shell.execute_reply.started":"2022-10-08T16:25:11.858375Z","shell.execute_reply":"2022-10-08T16:25:11.867613Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"X=np.array(x_original)\nprint(X.shape)\na=np.zeros((1,33))\n#print(X)\nX= np.insert(X, 0, 1, axis=2)\nprint(X[944])\nprint(X.shape)","metadata":{"id":"-uRTXSf6HKPa","outputId":"4b3f0909-fb6b-4b92-9803-dd250d9633bd","execution":{"iopub.status.busy":"2022-10-08T16:25:12.215542Z","iopub.execute_input":"2022-10-08T16:25:12.216223Z","iopub.status.idle":"2022-10-08T16:25:12.232409Z","shell.execute_reply.started":"2022-10-08T16:25:12.216185Z","shell.execute_reply":"2022-10-08T16:25:12.231137Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"(945, 32, 32)\n[[1.00000000e+000 1.00000000e+000 1.00000000e+000 ... 1.66022826e-024\n  4.29723780e-078 5.42851756e-139]\n [1.00000000e+000 1.00000000e+000 1.00000000e+000 ... 1.95725947e-003\n  6.60582554e-021 3.13988470e-072]\n [1.00000000e+000 1.00000000e+000 1.00000000e+000 ... 5.54645836e-003\n  2.18239727e-003 1.88687556e-014]\n ...\n [1.00000000e+000 1.00000000e+000 1.00000000e+000 ... 1.00000000e+000\n  1.00000000e+000 1.00000000e+000]\n [1.00000000e+000 1.00000000e+000 1.00000000e+000 ... 1.00000000e+000\n  1.00000000e+000 1.00000000e+000]\n [1.00000000e+000 1.00000000e+000 1.00000000e+000 ... 1.00000000e+000\n  1.00000000e+000 1.00000000e+000]]\n(945, 32, 33)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Creating zero valued multi- dimensional array of dimension (945*33*33) to accomdate Boundary conditions.\nX_data=np.zeros((945,33,33))\nprint(a.shape)\nfor i in range(945):\n    X_data[i]=np.append(X[i],a,axis=0)\n#print(X_data[944])\nprint(X_data.shape)","metadata":{"id":"fiOVXbkcJAJk","outputId":"6ca094ac-64d1-43be-d71d-0d6499bd1866","execution":{"iopub.status.busy":"2022-10-08T16:25:12.489677Z","iopub.execute_input":"2022-10-08T16:25:12.490503Z","iopub.status.idle":"2022-10-08T16:25:12.528458Z","shell.execute_reply.started":"2022-10-08T16:25:12.490463Z","shell.execute_reply":"2022-10-08T16:25:12.527614Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"(1, 33)\n(945, 33, 33)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Creating a channel for consideration of X-directional fixed BC\n\nX_data3=np.expand_dims(X_data,axis=3)\nprint(X_data3.shape)","metadata":{"id":"PQz5FeICK0FF","outputId":"060dffe6-02cc-44b4-b41b-c4ae542af426","execution":{"iopub.status.busy":"2022-10-08T16:25:12.719392Z","iopub.execute_input":"2022-10-08T16:25:12.720375Z","iopub.status.idle":"2022-10-08T16:25:12.727821Z","shell.execute_reply.started":"2022-10-08T16:25:12.720321Z","shell.execute_reply":"2022-10-08T16:25:12.726806Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"(945, 33, 33, 1)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Creating a channel for consideration of Y-directional fixed BC\n\nX_data4=X_data3\nprint(X_data4.shape)","metadata":{"id":"3uo2yG8uOVjj","outputId":"ebc666e1-be6d-4aa8-cecb-bc536f8b1e38","execution":{"iopub.status.busy":"2022-10-08T16:25:12.934079Z","iopub.execute_input":"2022-10-08T16:25:12.935019Z","iopub.status.idle":"2022-10-08T16:25:12.944864Z","shell.execute_reply.started":"2022-10-08T16:25:12.934976Z","shell.execute_reply":"2022-10-08T16:25:12.943787Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"(945, 33, 33, 1)\n","output_type":"stream"}]},{"cell_type":"code","source":"#Creating a channel for consideration of X-directional force BC\n\nX_data1_temp= np.insert(x_original, 0, 0, axis=2)\n#print(X_data1_temp[1])\n\nX_data1=np.zeros((945,33,33))\nfor i in range(945):\n    X_data1[i]=np.append(X_data1_temp[i],a,axis=0)\n#print(X_data1.shape)\n","metadata":{"id":"qsiys_l5RAin","execution":{"iopub.status.busy":"2022-10-08T16:25:13.116952Z","iopub.execute_input":"2022-10-08T16:25:13.117432Z","iopub.status.idle":"2022-10-08T16:25:13.152341Z","shell.execute_reply.started":"2022-10-08T16:25:13.117396Z","shell.execute_reply":"2022-10-08T16:25:13.151172Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#Creating a channel for consideration of Y-directional force BC\n\nX_data2 = X_data1\nfor i in range(945):\n    X_data2[i][32][32]=1\n\n#print(X_data2[944])\nX_data2=np.expand_dims(X_data2,axis=3)\nprint(X_data2.shape)","metadata":{"id":"XmXDNNK8VFT5","outputId":"7344e9de-3a30-408d-db81-158b68771aba","execution":{"iopub.status.busy":"2022-10-08T16:25:13.311130Z","iopub.execute_input":"2022-10-08T16:25:13.312327Z","iopub.status.idle":"2022-10-08T16:25:13.319740Z","shell.execute_reply.started":"2022-10-08T16:25:13.312286Z","shell.execute_reply":"2022-10-08T16:25:13.318283Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"(945, 33, 33, 1)\n","output_type":"stream"}]},{"cell_type":"code","source":"X_data1=np.expand_dims(X_data1,axis=3)\nprint(X_data1.shape)","metadata":{"id":"tdXEtJq3Rs0X","outputId":"1058e940-9121-43ac-bc0f-3f15a79ccdde","execution":{"iopub.status.busy":"2022-10-08T16:25:13.670382Z","iopub.execute_input":"2022-10-08T16:25:13.670760Z","iopub.status.idle":"2022-10-08T16:25:13.676931Z","shell.execute_reply.started":"2022-10-08T16:25:13.670722Z","shell.execute_reply":"2022-10-08T16:25:13.675212Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"(945, 33, 33, 1)\n","output_type":"stream"}]},{"cell_type":"code","source":"#Concatenating all the individual channels (BCs) into one, creating final input \n\nX_final= np.concatenate((X_data1, X_data2,X_data3,X_data4),axis=3)\nprint(X_final.shape)","metadata":{"id":"1ofVrD33UD7y","execution":{"iopub.status.busy":"2022-10-08T16:25:14.124649Z","iopub.execute_input":"2022-10-08T16:25:14.125342Z","iopub.status.idle":"2022-10-08T16:25:14.149957Z","shell.execute_reply.started":"2022-10-08T16:25:14.125307Z","shell.execute_reply":"2022-10-08T16:25:14.148781Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"(945, 33, 33, 4)\n","output_type":"stream"}]},{"cell_type":"code","source":"#Importing Volume fraction \n\nmy_data = genfromtxt('../input/dataset-new/input.csv', delimiter=',')[:,2]\nmy_data = my_data[0:945]\nprint(my_data[30:40])\n","metadata":{"id":"8gyLuOkYAFkL","outputId":"777c1e43-6524-4e10-ee7a-125adf32ecea","execution":{"iopub.status.busy":"2022-10-08T16:25:14.551381Z","iopub.execute_input":"2022-10-08T16:25:14.552537Z","iopub.status.idle":"2022-10-08T16:25:14.570107Z","shell.execute_reply.started":"2022-10-08T16:25:14.552496Z","shell.execute_reply":"2022-10-08T16:25:14.568991Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**CNN AUTOENCODER**","metadata":{"id":"fUG2oQibfEVV"}},{"cell_type":"code","source":"# Creating volume fraction data\n\nX_data_parent=np.ones((105,4,4,1))\n#print(X_data_parent)\na1 = X_data_parent*0.1\na2= X_data_parent*0.2\na3= X_data_parent*0.3\na4= X_data_parent*0.4\na5= X_data_parent*0.5\na6= X_data_parent*0.6\na7= X_data_parent*0.7\na8= X_data_parent*0.8\na9= X_data_parent*0.9\nabc=np.concatenate((a1,a2,a3,a4,a5,a6,a7,a8,a9),axis=0)\n\nprint(abc.shape)\n\n","metadata":{"id":"WZd-Hl0ccB6w","outputId":"5e26ba35-deb2-40f7-e5fe-620f82df4dfc","execution":{"iopub.status.busy":"2022-10-08T16:25:15.333961Z","iopub.execute_input":"2022-10-08T16:25:15.334317Z","iopub.status.idle":"2022-10-08T16:25:15.343541Z","shell.execute_reply.started":"2022-10-08T16:25:15.334287Z","shell.execute_reply":"2022-10-08T16:25:15.342445Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"(945, 4, 4, 1)\n","output_type":"stream"}]},{"cell_type":"code","source":"#Creating encoder-decoder model\n\nstart = Input(shape=(33, 33 ,4)) \n\n#ENCODER\nconv1_1= Conv2D(64,(2,2),activation ='relu',strides=(1,1))(start)\nconv1_2= Conv2D(64,(3,3),padding = \"same\",activation ='relu',strides=(1,1))(conv1_1)\npool1= MaxPooling2D(pool_size=(2,2))(conv1_2)\nconv1_3= Conv2D(128,(3,3),padding = \"same\",activation ='relu',strides=(1,1))(pool1)\nconv1_4= Conv2D(128,(3,3),padding = \"same\",activation ='relu',strides=(1,1))(conv1_3)\npool2= MaxPooling2D(pool_size=(2,2))(conv1_4)\nconv1_5= Conv2D(256,(3,3),padding = \"same\",activation ='relu',strides=(1,1))(pool2)\nconv1_6= Conv2D(256,(3,3),padding = \"same\",activation ='relu',strides=(1,1))(conv1_5)\npool3= MaxPooling2D(pool_size=(2,2))(conv1_6)\nh= Conv2D(512,(3,3),strides=(1, 1),padding = 'same',activation ='relu')(pool3)\n\nlabel=Input(shape=(4,4,1))\n# merge latent space with label\n#conv_ae.add(concatenate([my_data]))\nlat=Concatenate()([h,label])\n\n\n\n#DECODER\n\nconv2_1= Conv2D(512,(3,3),padding=\"same\",activation ='relu',strides=(1,1))(lat)\nup1= UpSampling2D(size=(2,2))(conv2_1)\nconv2_2= Conv2D(256,(3,3),padding = \"same\",activation ='relu',strides=(1,1))(up1)\nconv2_3= Conv2D(256,(3,3),padding = \"same\",activation ='relu',strides=(1,1))(conv2_2)\nup2= UpSampling2D(size=(2,2))(conv2_3)\nconv2_4= Conv2D(128,(3,3),padding = \"same\",activation ='relu',strides=(1,1))(up2)\nconv2_5= Conv2D(128,(3,3),padding = \"same\",activation ='relu',strides=(1,1))(conv2_4)\nup3= UpSampling2D(size=(2,2))(conv2_5)\nconv2_6= Conv2D(64,(3,3),padding = \"same\",activation ='relu',strides=(1,1))(up3)\nconv2_7= Conv2D(2,(3,3),padding = \"same\",activation ='relu',strides=(1,1))(conv2_6)\nr= Conv2D(1,(3,3),padding = \"same\",activation ='sigmoid',strides=(1,1))(conv2_7)\n\n","metadata":{"id":"CULjaOSo4vhN","execution":{"iopub.status.busy":"2022-10-08T16:25:16.077422Z","iopub.execute_input":"2022-10-08T16:25:16.077801Z","iopub.status.idle":"2022-10-08T16:25:19.643165Z","shell.execute_reply.started":"2022-10-08T16:25:16.077769Z","shell.execute_reply":"2022-10-08T16:25:19.642052Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"2022-10-08 16:25:16.176253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-10-08 16:25:16.284939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-10-08 16:25:16.285667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-10-08 16:25:16.287053: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-10-08 16:25:16.287370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-10-08 16:25:16.288114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-10-08 16:25:16.288790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-10-08 16:25:19.115852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-10-08 16:25:19.116792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-10-08 16:25:19.117567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-10-08 16:25:19.118210: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15401 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}]},{"cell_type":"code","source":"#Assigning inputs and outputs to the autoencoder model and compiling the same\n\nautoencoder = Model(inputs=[start,label], outputs=r)\nautoencoder.compile(optimizer='adam', loss='mae')\nautoencoder.summary()","metadata":{"id":"jlPZumBejSLX","outputId":"f4e79f8d-f0eb-4185-d7c9-773b21be5e76","execution":{"iopub.status.busy":"2022-10-08T16:25:19.645036Z","iopub.execute_input":"2022-10-08T16:25:19.645959Z","iopub.status.idle":"2022-10-08T16:25:19.669569Z","shell.execute_reply.started":"2022-10-08T16:25:19.645922Z","shell.execute_reply":"2022-10-08T16:25:19.668734Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 33, 33, 4)]  0                                            \n__________________________________________________________________________________________________\nconv2d (Conv2D)                 (None, 32, 32, 64)   1088        input_1[0][0]                    \n__________________________________________________________________________________________________\nconv2d_1 (Conv2D)               (None, 32, 32, 64)   36928       conv2d[0][0]                     \n__________________________________________________________________________________________________\nmax_pooling2d (MaxPooling2D)    (None, 16, 16, 64)   0           conv2d_1[0][0]                   \n__________________________________________________________________________________________________\nconv2d_2 (Conv2D)               (None, 16, 16, 128)  73856       max_pooling2d[0][0]              \n__________________________________________________________________________________________________\nconv2d_3 (Conv2D)               (None, 16, 16, 128)  147584      conv2d_2[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_1 (MaxPooling2D)  (None, 8, 8, 128)    0           conv2d_3[0][0]                   \n__________________________________________________________________________________________________\nconv2d_4 (Conv2D)               (None, 8, 8, 256)    295168      max_pooling2d_1[0][0]            \n__________________________________________________________________________________________________\nconv2d_5 (Conv2D)               (None, 8, 8, 256)    590080      conv2d_4[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_2 (MaxPooling2D)  (None, 4, 4, 256)    0           conv2d_5[0][0]                   \n__________________________________________________________________________________________________\nconv2d_6 (Conv2D)               (None, 4, 4, 512)    1180160     max_pooling2d_2[0][0]            \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            [(None, 4, 4, 1)]    0                                            \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 4, 4, 513)    0           conv2d_6[0][0]                   \n                                                                 input_2[0][0]                    \n__________________________________________________________________________________________________\nconv2d_7 (Conv2D)               (None, 4, 4, 512)    2364416     concatenate[0][0]                \n__________________________________________________________________________________________________\nup_sampling2d (UpSampling2D)    (None, 8, 8, 512)    0           conv2d_7[0][0]                   \n__________________________________________________________________________________________________\nconv2d_8 (Conv2D)               (None, 8, 8, 256)    1179904     up_sampling2d[0][0]              \n__________________________________________________________________________________________________\nconv2d_9 (Conv2D)               (None, 8, 8, 256)    590080      conv2d_8[0][0]                   \n__________________________________________________________________________________________________\nup_sampling2d_1 (UpSampling2D)  (None, 16, 16, 256)  0           conv2d_9[0][0]                   \n__________________________________________________________________________________________________\nconv2d_10 (Conv2D)              (None, 16, 16, 128)  295040      up_sampling2d_1[0][0]            \n__________________________________________________________________________________________________\nconv2d_11 (Conv2D)              (None, 16, 16, 128)  147584      conv2d_10[0][0]                  \n__________________________________________________________________________________________________\nup_sampling2d_2 (UpSampling2D)  (None, 32, 32, 128)  0           conv2d_11[0][0]                  \n__________________________________________________________________________________________________\nconv2d_12 (Conv2D)              (None, 32, 32, 64)   73792       up_sampling2d_2[0][0]            \n__________________________________________________________________________________________________\nconv2d_13 (Conv2D)              (None, 32, 32, 2)    1154        conv2d_12[0][0]                  \n__________________________________________________________________________________________________\nconv2d_14 (Conv2D)              (None, 32, 32, 1)    19          conv2d_13[0][0]                  \n==================================================================================================\nTotal params: 6,976,853\nTrainable params: 6,976,853\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"# test train split of training and validation data\n\nimages_train, images_test,abc_train,abc_test = train_test_split(X_final,abc,train_size= 0.8,test_size=0.20,shuffle=False)\nprint(images_train.shape)\nprint(abc_train.shape)\n\nX_input_train = X_input[:756]\nprint(X_input_train.shape)\nX_input_test = X_input[756:]\nprint(X_input_test.shape)\n\n\n\n","metadata":{"id":"1H4i9PoV7HWF","outputId":"a379ccf0-35dc-46b3-a077-f27fd6604d82","execution":{"iopub.status.busy":"2022-10-08T16:25:19.671670Z","iopub.execute_input":"2022-10-08T16:25:19.672830Z","iopub.status.idle":"2022-10-08T16:25:19.693226Z","shell.execute_reply.started":"2022-10-08T16:25:19.672795Z","shell.execute_reply":"2022-10-08T16:25:19.692095Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"(756, 33, 33, 4)\n(756, 4, 4, 1)\n(756, 32, 32)\n(189, 32, 32)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Assigning Hyperparameters (batch size and epochs) and training the model\n\nepochs = 12\nbatch_size = 50\n\nhistory = autoencoder.fit([images_train,abc_train], X_input_train , batch_size=batch_size, epochs=epochs,verbose=1, validation_data=([images_test,abc_test],X_input_test))","metadata":{"id":"AnZejjSn4fnv","outputId":"c5fa53f6-528a-4295-9cc2-7724f25a71da","execution":{"iopub.status.busy":"2022-10-08T16:25:19.695438Z","iopub.execute_input":"2022-10-08T16:25:19.695922Z","iopub.status.idle":"2022-10-08T16:25:33.558133Z","shell.execute_reply.started":"2022-10-08T16:25:19.695886Z","shell.execute_reply":"2022-10-08T16:25:33.557139Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Epoch 1/12\n","output_type":"stream"},{"name":"stderr","text":"2022-10-08 16:25:19.809159: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n2022-10-08 16:25:21.708242: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n","output_type":"stream"},{"name":"stdout","text":"16/16 [==============================] - 9s 80ms/step - loss: 0.2936 - val_loss: 0.1265\nEpoch 2/12\n16/16 [==============================] - 0s 23ms/step - loss: 0.1639 - val_loss: 0.1248\nEpoch 3/12\n16/16 [==============================] - 0s 23ms/step - loss: 0.0684 - val_loss: 0.1096\nEpoch 4/12\n16/16 [==============================] - 0s 23ms/step - loss: 0.0649 - val_loss: 0.0491\nEpoch 5/12\n16/16 [==============================] - 0s 23ms/step - loss: 0.0450 - val_loss: 0.0537\nEpoch 6/12\n16/16 [==============================] - 0s 23ms/step - loss: 0.0295 - val_loss: 0.0309\nEpoch 7/12\n16/16 [==============================] - 0s 23ms/step - loss: 0.0249 - val_loss: 0.0448\nEpoch 8/12\n16/16 [==============================] - 0s 25ms/step - loss: 0.0219 - val_loss: 0.0439\nEpoch 9/12\n16/16 [==============================] - 0s 23ms/step - loss: 0.0215 - val_loss: 0.0257\nEpoch 10/12\n16/16 [==============================] - 0s 26ms/step - loss: 0.0187 - val_loss: 0.0337\nEpoch 11/12\n16/16 [==============================] - 0s 24ms/step - loss: 0.0194 - val_loss: 0.0387\nEpoch 12/12\n16/16 [==============================] - 0s 23ms/step - loss: 0.0167 - val_loss: 0.0343\n","output_type":"stream"}]},{"cell_type":"markdown","source":"###### Predicting on test data\n\ndecoded_imgs = autoencoder.predict([images_test, abc_test])\n\n","metadata":{"id":"gmnsEgP2k0ki","execution":{"iopub.status.busy":"2022-10-08T13:16:07.627610Z","iopub.execute_input":"2022-10-08T13:16:07.627981Z","iopub.status.idle":"2022-10-08T13:16:08.272027Z","shell.execute_reply.started":"2022-10-08T13:16:07.627944Z","shell.execute_reply":"2022-10-08T13:16:08.270898Z"}}},{"cell_type":"code","source":"decoded_imgs = autoencoder.predict([images_test, abc_test])","metadata":{"execution":{"iopub.status.busy":"2022-10-08T16:25:35.968622Z","iopub.execute_input":"2022-10-08T16:25:35.969065Z","iopub.status.idle":"2022-10-08T16:25:36.616701Z","shell.execute_reply.started":"2022-10-08T16:25:35.969032Z","shell.execute_reply":"2022-10-08T16:25:36.615711Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"#Plotting interactive images of predicted images on test_data\n\n\ndef plot_bridge(i):\n    fig,ax = pyplot.subplots()\n    \n    #print(f\"Volume fraction: {y[i]}\")\n    im = pyplot.imshow(decoded_imgs[i].reshape(32,32), cmap='gray_r')\n    pyplot.axis('off')\n    \n    pyplot.show()\n\ninteract(plot_bridge, i=(0, len(decoded_imgs)-1))\n\n\n","metadata":{"id":"phLMdnLQraSp","outputId":"01d43a2e-33d5-472f-de91-f6f702e6da3e","execution":{"iopub.status.busy":"2022-10-08T16:25:38.983309Z","iopub.execute_input":"2022-10-08T16:25:38.983732Z","iopub.status.idle":"2022-10-08T16:25:39.118399Z","shell.execute_reply.started":"2022-10-08T16:25:38.983671Z","shell.execute_reply":"2022-10-08T16:25:39.117179Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"interactive(children=(IntSlider(value=94, description='i', max=188), Output()), _dom_classes=('widget-interactâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"499842059edc488ba653fe3f406e7d49"}},"metadata":{}},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"<function __main__.plot_bridge(i)>"},"metadata":{}}]},{"cell_type":"code","source":"#Plotting interactive images of original images(ground truth) on test_data\n\n\ndef plot_bridge1(i):\n    fig,ax = pyplot.subplots()\n    \n    #print(f\"Volume fraction: {y[i]}\")\n    im1 = pyplot.imshow(X_input_test[i].reshape(32,32), cmap='gray_r')\n    pyplot.axis('off')\n    \n    pyplot.show()\n \n \ninteract(plot_bridge1, i=(0, len(X_input_test)-1))","metadata":{"id":"rLlcrBUosQKV","outputId":"bf7275bd-101f-44d8-b9e6-5cbae1ba5b2d","execution":{"iopub.status.busy":"2022-10-08T16:25:47.210113Z","iopub.execute_input":"2022-10-08T16:25:47.210520Z","iopub.status.idle":"2022-10-08T16:25:47.333126Z","shell.execute_reply.started":"2022-10-08T16:25:47.210489Z","shell.execute_reply":"2022-10-08T16:25:47.331924Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"interactive(children=(IntSlider(value=94, description='i', max=188), Output()), _dom_classes=('widget-interactâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c86ed9f7cf7479688a777890e510a09"}},"metadata":{}},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"<function __main__.plot_bridge1(i)>"},"metadata":{}}]},{"cell_type":"code","source":"#Plotting training and validation losses against epoch\n\nprint(history.history.keys())\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper right')\nplt.show()","metadata":{"id":"29Jg9G3Y76a2","outputId":"b9a3e28a-a3c7-4525-cd35-e2415e5b363b","execution":{"iopub.status.busy":"2022-10-08T16:25:47.711000Z","iopub.execute_input":"2022-10-08T16:25:47.711329Z","iopub.status.idle":"2022-10-08T16:25:47.922597Z","shell.execute_reply.started":"2022-10-08T16:25:47.711300Z","shell.execute_reply":"2022-10-08T16:25:47.921651Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"dict_keys(['loss', 'val_loss'])\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"## GAN ","metadata":{"id":"6hI_BiwzsTVU"}},{"cell_type":"markdown","source":"Generator module\n","metadata":{"id":"GzgestIbsaIW"}},{"cell_type":"code","source":"#Importing all the necessary modules and routines for GAN\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom PIL import Image\nfrom PIL.ImageColor import getrgb\nimport glob\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.callbacks import TensorBoard\nfrom tensorflow.keras.layers import BatchNormalization, Activation, LeakyReLU, Add, Dense, Flatten\nfrom tensorflow.keras.layers import Conv2D, UpSampling2D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nimport shutil\nimport datetime\n%load_ext tensorboard","metadata":{"id":"GhVJg1cRshFe","execution":{"iopub.status.busy":"2022-10-08T16:25:49.757284Z","iopub.execute_input":"2022-10-08T16:25:49.758041Z","iopub.status.idle":"2022-10-08T16:25:49.773286Z","shell.execute_reply.started":"2022-10-08T16:25:49.757996Z","shell.execute_reply":"2022-10-08T16:25:49.772374Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"#Predicting on full data set as it will be input to GAN\n\ninput_GAN = autoencoder.predict([X_final,abc])\n\ndef plot_bridge3(i):\n    fig,ax = pyplot.subplots()\n    \n    #print(f\"Volume fraction: {y[i]}\")\n    im3 = pyplot.imshow(input_GAN[i].reshape(32,32), cmap='gray_r')\n    pyplot.axis('off')\n    \n    pyplot.show()\n \n \ninteract(plot_bridge3, i=(0, len(input_GAN)-1))","metadata":{"id":"Ljzy6CDPvzjH","execution":{"iopub.status.busy":"2022-10-08T16:25:50.370074Z","iopub.execute_input":"2022-10-08T16:25:50.370427Z","iopub.status.idle":"2022-10-08T16:25:51.210264Z","shell.execute_reply.started":"2022-10-08T16:25:50.370398Z","shell.execute_reply":"2022-10-08T16:25:51.209041Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"interactive(children=(IntSlider(value=472, description='i', max=944), Output()), _dom_classes=('widget-interacâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29117f65a6bc456aa9b460ff0ee758c3"}},"metadata":{}},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"<function __main__.plot_bridge3(i)>"},"metadata":{}}]},{"cell_type":"code","source":"#Building generator network\n\n\ndef build_generator():\n   \n    residual_blocks = 16\n    input_shape = (32, 32, 1)\n\n    # Input Layer of the generator network\n    input_layer = Input(shape=input_shape)\n\n    # Add the pre-residual block\n    conv1_GeN = Conv2D(64,(5,5),activation ='relu',padding='same',strides=(1,1))(input_layer)\n\n    \n\n    # Add the post-residual block\n\n    conv2_GeN= Conv2D(64,(3,3),padding = \"same\",activation ='relu',strides=(1,1))(conv1_GeN)\n    conv3_GeN= Conv2D(64,(3,3),padding = \"same\",activation ='relu',strides=(1,1))(conv2_GeN)\n    conv4_GeN= Conv2D(64,(3,3),padding = \"same\",activation ='relu',strides=(1,1))(conv3_GeN)\n    conv5_GeN= Conv2D(64,(3,3),padding = \"same\",activation ='relu',strides=(1,1))(conv4_GeN)\n    conv6_GeN= Conv2D(64,(3,3),padding = \"same\",activation ='relu',strides=(1,1))(conv5_GeN)\n    conv7_GeN= Conv2D(64,(3,3),padding = \"same\",activation ='relu',strides=(1,1))(conv6_GeN)\n    conv8_GeN= Conv2D(64,(3,3),padding = \"same\",activation ='relu',strides=(1,1))(conv7_GeN)\n    conv9_GeN= Conv2D(64,(3,3),padding = \"same\",activation ='relu',strides=(1,1))(conv8_GeN)\n\n    conv10_GeN = Conv2D(64,(3,3),padding = \"same\",activation ='relu',strides=(1,1))(conv9_GeN)\n    Residual_inter_output= BatchNormalization(momentum = 0.8)(conv10_GeN)\n\n    # Take the sum of the output from the pre-residual block(gen1) and the post-residual block(gen2)\n    Residual_output = Add()([Residual_inter_output, conv1_GeN])\n\n    # Add an upsampling block\n    conv11_GeN = UpSampling2D(size=(2,2))(Residual_output) \n    conv12_GeN= Conv2D(256,(3,3),padding = \"same\",activation ='relu',strides=(1,1))(conv11_GeN)\n    \n    \n    # Add another upsampling block\n    conv13_GeN = UpSampling2D(size=(2,2))(conv12_GeN)\n    conv14_GeN= Conv2D(256,(3,3),padding = \"same\",activation ='relu',strides=(1,1))(conv13_GeN)\n\n    # Output convolution layer\n    conv15_GeN= Conv2D(1,(5,5),padding = \"same\",activation ='sigmoid',strides=(1,1))(conv14_GeN)\n\n\n    # Keras model\n    model = Model(inputs=[input_layer], outputs=[conv15_GeN], name='generator')\n    return model","metadata":{"id":"iF1usoMCDEst","execution":{"iopub.status.busy":"2022-10-08T16:25:56.518536Z","iopub.execute_input":"2022-10-08T16:25:56.518902Z","iopub.status.idle":"2022-10-08T16:25:56.532091Z","shell.execute_reply.started":"2022-10-08T16:25:56.518872Z","shell.execute_reply":"2022-10-08T16:25:56.530822Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"#Building discriminator network\n\ndef build_discriminator():\n  \n    leakyrelu_alpha = 0.2\n    input_shape = (128,128,2)\n\n    input_layer = Input(shape=input_shape)\n\n    # Add the first convolution block\n    dis1 = Conv2D(64,(3,3), strides=1, padding='same')(input_layer)\n    dis1 = LeakyReLU(alpha=leakyrelu_alpha)(dis1)\n\n    # Add the 2nd convolution block\n    dis2 = Conv2D(64,(3,3), strides=2, padding='same')(dis1)\n    dis2 = LeakyReLU(alpha=leakyrelu_alpha)(dis2)\n    dis2 = BatchNormalization(momentum = 0.8)(dis2)\n\n    # Add the third convolution block\n    dis3 = Conv2D(128,(3,3), strides=1,padding = 'same')(dis2)\n    dis3 = LeakyReLU(alpha=leakyrelu_alpha)(dis3)\n    dis3 = BatchNormalization(momentum = 0.8)(dis3)\n\n    # Add the fourth convolution block\n    dis4 = Conv2D(128,(3,3), strides=2, padding = 'same')(dis3)\n    dis4 = LeakyReLU(alpha=leakyrelu_alpha)(dis4)\n    dis4 = BatchNormalization(momentum = 0.8)(dis4)\n\n    # Add the fifth convolution block\n    dis5 = Conv2D(256,(3,3), strides=1,padding='same')(dis4)\n    dis5 = LeakyReLU(alpha=leakyrelu_alpha)(dis5)\n    dis5 = BatchNormalization(momentum = 0.8)(dis5)\n\n    # Add the sixth convolution block\n    dis6 = Conv2D(256,(3,3), strides=2, padding = 'same')(dis5)\n    dis6 = LeakyReLU(alpha=leakyrelu_alpha)(dis6)\n    dis6 = BatchNormalization(momentum = 0.8)(dis6)\n\n    # Add the seventh convolution block\n    dis7 = Conv2D(512,(3,3), strides=1,padding='same')(dis6)\n    dis7 = LeakyReLU(alpha=leakyrelu_alpha)(dis7)\n    dis7 = BatchNormalization(momentum = 0.8)(dis7)\n\n    # Add the eight convolution block\n    dis8 = Conv2D(512,(3,3), strides=2, padding = 'same')(dis7)\n    dis8 = LeakyReLU(alpha=leakyrelu_alpha)(dis8)\n    dis8 = BatchNormalization(momentum = 0.8)(dis8)\n\n    # Add a dense layer\n    #avgd = keras.layers.AveragePooling2D(pool_size=(4,4) , strides = (4,4))(dis8)\n\n    flat = keras.layers.Flatten()(dis8)\n    dis9 = Dense(units=1024)(flat)\n    dis9 = LeakyReLU(alpha=0.2)(dis9)\n\n    # Last dense layer - for classification\n    output = Dense(units=1, activation='sigmoid')(dis9)\n\n    model = Model(inputs=[input_layer], outputs=[output], name='discriminator')\n    return model","metadata":{"id":"YtV2vIrYHEMK","execution":{"iopub.status.busy":"2022-10-08T16:25:58.169982Z","iopub.execute_input":"2022-10-08T16:25:58.170347Z","iopub.status.idle":"2022-10-08T16:25:58.184119Z","shell.execute_reply.started":"2022-10-08T16:25:58.170316Z","shell.execute_reply":"2022-10-08T16:25:58.183135Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Importing Pretrained weights of Imagenet data set in VGG19 architecture at 13th layer\n\ndef build_vgg():\n    \"\"\"\n    Build VGG network to extract image features\n    \"\"\"\n    grayImage = (128,128,3)\n    #dummy_RGB_images[:, :, :, 0] = train_gray_images[:, :, :, 0]\n#dummy_RGB_images[:, :, :, 1] = train_gray_images[:, :, :, 0]\n#dummy_RGB_images[:, :, :, 2] = train_gray_images[:, :, :, 0]\n\n    vgg = keras.applications.VGG19(include_top = False ,  input_shape = grayImage , weights=\"imagenet\")\n    features = vgg.get_layer(index = 13).output\n    #features = vgg.output\n    model = keras.Model(inputs=[vgg.inputs], outputs=[features])\n    return model\n\n        \n\n\n","metadata":{"id":"jNb3mCHWwPiQ","execution":{"iopub.status.busy":"2022-10-08T16:25:59.060596Z","iopub.execute_input":"2022-10-08T16:25:59.061302Z","iopub.status.idle":"2022-10-08T16:25:59.068384Z","shell.execute_reply.started":"2022-10-08T16:25:59.061267Z","shell.execute_reply":"2022-10-08T16:25:59.067231Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{"id":"t0OYGwgTQllG"}},{"cell_type":"code","source":"# Building Adverserial (combination of gnerator and discriminator) network\n\n\nmode = 'train'\n\nepochs = 30\nbatch_size = 20\n\ncommon_optimizer = Adam(0.0002, 0.5)\nlow_resolution_shape = (32, 32, 1)\nhigh_resolution_shape = (128, 128,1)\n\n\n\n\n#rgbImage=np.concatenate((high_resolution_shape,high_resolution_shape,high_resolution_shape))\n\n\n#print(rgbImage.shape)\n\nvgg = build_vgg()\nvgg.trainable = False\nvgg.compile(loss='mae', optimizer=common_optimizer, metrics=['accuracy'])\n\n# Build and compile the discriminator network\ndiscriminator = build_discriminator()\ndiscriminator.compile(loss='mae', optimizer=common_optimizer, metrics=['accuracy'])\n\n# Build the generator network\ngenerator = build_generator()\n\n\"\"\"\nBuild and compile the adversarial model\n\"\"\"\n\n# Input layers for high-resolution and low-resolution images\ninput_high_resolution = Input(shape=high_resolution_shape)\ninput_low_resolution = Input(shape=low_resolution_shape)\n#Input_final_to_discriminator=Input(shape=Input_to_discriminator)\n\n\n\n# Generate high-resolution images from low-resolution images\ngenerated_high_resolution_images = generator(input_low_resolution)\n\n\n\nrgbImage=Concatenate()([generated_high_resolution_images,generated_high_resolution_images,generated_high_resolution_images])\n\n# Extract feature maps of the generated images\nfeatures = vgg(rgbImage)\n\n# Freeze the discriminator for while training the adversarial model\ndiscriminator.trainable = False\n\nfinal_input_to_discriminator=Concatenate()([input_high_resolution,input_high_resolution])\n# Get the probability of generated high-resolution images\nprobs = discriminator(final_input_to_discriminator)\n\n\n\n# Create and compile an adversarial model\nadversarial_model = Model([input_low_resolution, input_high_resolution], [probs,features])\nadversarial_model.compile(loss=['binary_crossentropy', 'mae'], loss_weights=[1e-3, 1], optimizer=common_optimizer)\n\ngenerator.summary()\ndiscriminator.summary()\nadversarial_model.summary()","metadata":{"id":"Q5RGUSet8oT1","outputId":"993823be-651f-49be-ccc0-c2c479e96c84","execution":{"iopub.status.busy":"2022-10-08T17:17:14.073103Z","iopub.execute_input":"2022-10-08T17:17:14.073888Z","iopub.status.idle":"2022-10-08T17:17:14.832279Z","shell.execute_reply.started":"2022-10-08T17:17:14.073846Z","shell.execute_reply":"2022-10-08T17:17:14.831294Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stdout","text":"Model: \"generator\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_45 (InputLayer)           [(None, 32, 32, 1)]  0                                            \n__________________________________________________________________________________________________\nconv2d_191 (Conv2D)             (None, 32, 32, 64)   1664        input_45[0][0]                   \n__________________________________________________________________________________________________\nconv2d_192 (Conv2D)             (None, 32, 32, 64)   36928       conv2d_191[0][0]                 \n__________________________________________________________________________________________________\nconv2d_193 (Conv2D)             (None, 32, 32, 64)   36928       conv2d_192[0][0]                 \n__________________________________________________________________________________________________\nconv2d_194 (Conv2D)             (None, 32, 32, 64)   36928       conv2d_193[0][0]                 \n__________________________________________________________________________________________________\nconv2d_195 (Conv2D)             (None, 32, 32, 64)   36928       conv2d_194[0][0]                 \n__________________________________________________________________________________________________\nconv2d_196 (Conv2D)             (None, 32, 32, 64)   36928       conv2d_195[0][0]                 \n__________________________________________________________________________________________________\nconv2d_197 (Conv2D)             (None, 32, 32, 64)   36928       conv2d_196[0][0]                 \n__________________________________________________________________________________________________\nconv2d_198 (Conv2D)             (None, 32, 32, 64)   36928       conv2d_197[0][0]                 \n__________________________________________________________________________________________________\nconv2d_199 (Conv2D)             (None, 32, 32, 64)   36928       conv2d_198[0][0]                 \n__________________________________________________________________________________________________\nconv2d_200 (Conv2D)             (None, 32, 32, 64)   36928       conv2d_199[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_71 (BatchNo (None, 32, 32, 64)   256         conv2d_200[0][0]                 \n__________________________________________________________________________________________________\nadd_8 (Add)                     (None, 32, 32, 64)   0           batch_normalization_71[0][0]     \n                                                                 conv2d_191[0][0]                 \n__________________________________________________________________________________________________\nup_sampling2d_19 (UpSampling2D) (None, 64, 64, 64)   0           add_8[0][0]                      \n__________________________________________________________________________________________________\nconv2d_201 (Conv2D)             (None, 64, 64, 256)  147712      up_sampling2d_19[0][0]           \n__________________________________________________________________________________________________\nup_sampling2d_20 (UpSampling2D) (None, 128, 128, 256 0           conv2d_201[0][0]                 \n__________________________________________________________________________________________________\nconv2d_202 (Conv2D)             (None, 128, 128, 256 590080      up_sampling2d_20[0][0]           \n__________________________________________________________________________________________________\nconv2d_203 (Conv2D)             (None, 128, 128, 1)  6401        conv2d_202[0][0]                 \n==================================================================================================\nTotal params: 1,078,465\nTrainable params: 1,078,337\nNon-trainable params: 128\n__________________________________________________________________________________________________\nModel: \"discriminator\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_44 (InputLayer)        [(None, 128, 128, 2)]     0         \n_________________________________________________________________\nconv2d_183 (Conv2D)          (None, 128, 128, 64)      1216      \n_________________________________________________________________\nleaky_re_lu_72 (LeakyReLU)   (None, 128, 128, 64)      0         \n_________________________________________________________________\nconv2d_184 (Conv2D)          (None, 64, 64, 64)        36928     \n_________________________________________________________________\nleaky_re_lu_73 (LeakyReLU)   (None, 64, 64, 64)        0         \n_________________________________________________________________\nbatch_normalization_64 (Batc (None, 64, 64, 64)        256       \n_________________________________________________________________\nconv2d_185 (Conv2D)          (None, 64, 64, 128)       73856     \n_________________________________________________________________\nleaky_re_lu_74 (LeakyReLU)   (None, 64, 64, 128)       0         \n_________________________________________________________________\nbatch_normalization_65 (Batc (None, 64, 64, 128)       512       \n_________________________________________________________________\nconv2d_186 (Conv2D)          (None, 32, 32, 128)       147584    \n_________________________________________________________________\nleaky_re_lu_75 (LeakyReLU)   (None, 32, 32, 128)       0         \n_________________________________________________________________\nbatch_normalization_66 (Batc (None, 32, 32, 128)       512       \n_________________________________________________________________\nconv2d_187 (Conv2D)          (None, 32, 32, 256)       295168    \n_________________________________________________________________\nleaky_re_lu_76 (LeakyReLU)   (None, 32, 32, 256)       0         \n_________________________________________________________________\nbatch_normalization_67 (Batc (None, 32, 32, 256)       1024      \n_________________________________________________________________\nconv2d_188 (Conv2D)          (None, 16, 16, 256)       590080    \n_________________________________________________________________\nleaky_re_lu_77 (LeakyReLU)   (None, 16, 16, 256)       0         \n_________________________________________________________________\nbatch_normalization_68 (Batc (None, 16, 16, 256)       1024      \n_________________________________________________________________\nconv2d_189 (Conv2D)          (None, 16, 16, 512)       1180160   \n_________________________________________________________________\nleaky_re_lu_78 (LeakyReLU)   (None, 16, 16, 512)       0         \n_________________________________________________________________\nbatch_normalization_69 (Batc (None, 16, 16, 512)       2048      \n_________________________________________________________________\nconv2d_190 (Conv2D)          (None, 8, 8, 512)         2359808   \n_________________________________________________________________\nleaky_re_lu_79 (LeakyReLU)   (None, 8, 8, 512)         0         \n_________________________________________________________________\nbatch_normalization_70 (Batc (None, 8, 8, 512)         2048      \n_________________________________________________________________\nflatten_8 (Flatten)          (None, 32768)             0         \n_________________________________________________________________\ndense_16 (Dense)             (None, 1024)              33555456  \n_________________________________________________________________\nleaky_re_lu_80 (LeakyReLU)   (None, 1024)              0         \n_________________________________________________________________\ndense_17 (Dense)             (None, 1)                 1025      \n=================================================================\nTotal params: 38,248,705\nTrainable params: 0\nNon-trainable params: 38,248,705\n_________________________________________________________________\nModel: \"model_18\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_47 (InputLayer)           [(None, 32, 32, 1)]  0                                            \n__________________________________________________________________________________________________\ninput_46 (InputLayer)           [(None, 128, 128, 1) 0                                            \n__________________________________________________________________________________________________\ngenerator (Functional)          (None, 128, 128, 1)  1078465     input_47[0][0]                   \n__________________________________________________________________________________________________\nconcatenate_18 (Concatenate)    (None, 128, 128, 2)  0           input_46[0][0]                   \n                                                                 input_46[0][0]                   \n__________________________________________________________________________________________________\nconcatenate_17 (Concatenate)    (None, 128, 128, 3)  0           generator[0][0]                  \n                                                                 generator[0][0]                  \n                                                                 generator[0][0]                  \n__________________________________________________________________________________________________\ndiscriminator (Functional)      (None, 1)            38248705    concatenate_18[0][0]             \n__________________________________________________________________________________________________\nmodel_17 (Functional)           (None, 16, 16, 512)  5865536     concatenate_17[0][0]             \n==================================================================================================\nTotal params: 45,192,706\nTrainable params: 1,078,337\nNon-trainable params: 44,114,369\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"#Importing high reolution images(ground truth) and splitting data for training and testing\n\n\nlow_resolution_images = input_GAN\ninverted_high_resolution_img =np.load('../input/dataset-new/High_res.npy',allow_pickle=True)\n\n\n#inverted_high_resolution_img = np.invert(high_resolution_images)\n\nlow_resolution_images_train,low_resolution_images_test,inverted_high_resolution_img_train,inverted_high_resolution_img_test = train_test_split(low_resolution_images,inverted_high_resolution_img,train_size= 0.8,test_size=0.20,shuffle=False)\nprint(low_resolution_images_train.shape)\nprint(inverted_high_resolution_img_test.shape)\nprint(low_resolution_images.shape)","metadata":{"id":"hDRAId1u_-Hs","outputId":"1a45c877-66de-4b1c-b9f3-b70ae13d6dbe","execution":{"iopub.status.busy":"2022-10-08T17:17:14.834173Z","iopub.execute_input":"2022-10-08T17:17:14.835100Z","iopub.status.idle":"2022-10-08T17:17:14.877997Z","shell.execute_reply.started":"2022-10-08T17:17:14.835062Z","shell.execute_reply":"2022-10-08T17:17:14.876962Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"(756, 32, 32, 1)\n(189, 128, 128)\n(945, 32, 32, 1)\n","output_type":"stream"}]},{"cell_type":"code","source":"#temporary modification of ground truth data to be passed in VGG19 network\n\n\ninverted_high_resolution_img1=np.expand_dims(inverted_high_resolution_img_train,axis=3)\ninverted_high_resolution_img2=np.concatenate((inverted_high_resolution_img1,inverted_high_resolution_img1,inverted_high_resolution_img1),axis=3)\n\n\n#UPSCALING OF 32x32: \nresized_low_resolution_image = resize(low_resolution_images, (945,128,128,1),order =0,preserve_range=True, anti_aliasing=False)\nprint(resized_low_resolution_image.shape)\nprint(inverted_high_resolution_img.shape)\nexpanded = np.expand_dims(inverted_high_resolution_img,axis=3)\ndiscrim_realdata= np.concatenate((resized_low_resolution_image,expanded),axis=3)\nprint(discrim_realdata.shape)\ndiscrim_realdata_train=discrim_realdata[:756]\nprint(discrim_realdata_train.shape)\ndiscrim_realdata_test=discrim_realdata[756:]\nprint(discrim_realdata_test.shape)\n\n#SPLIT THE RESIZED DATA TO TRAIN AND TEST\nresized_low_resolution_image_train = resized_low_resolution_image[:756]\nresized_low_resolution_image_test = resized_low_resolution_image[756:]\nprint(resized_low_resolution_image_train.shape)\nprint(resized_low_resolution_image_test.shape)\n\n\n#def plot_bridge7(i):\n#    fig,ax = pyplot.subplots()\n#    \n#   #print(f\"Volume fraction: {y[i]}\")\n#    im7 = pyplot.imshow(inverted_high_resolution_img[i].reshape(128,128), cmap='gray_r')\n#    pyplot.axis('off')\n#    \n#    pyplot.show()\n \n \n \n#interact(plot_bridge7, i=(0, len(inverted_high_resolution_img)-1))\n#print(discrim_realdata[0:1])","metadata":{"id":"msChHHxLF8aL","outputId":"5c0b3390-440e-4cce-a1e0-ea13554fb52a","execution":{"iopub.status.busy":"2022-10-08T17:17:14.879297Z","iopub.execute_input":"2022-10-08T17:17:14.880238Z","iopub.status.idle":"2022-10-08T17:17:15.263142Z","shell.execute_reply.started":"2022-10-08T17:17:14.880190Z","shell.execute_reply":"2022-10-08T17:17:15.262054Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"(945, 128, 128, 1)\n(945, 128, 128)\n(945, 128, 128, 2)\n(756, 128, 128, 2)\n(189, 128, 128, 2)\n(756, 128, 128, 1)\n(189, 128, 128, 1)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Training Adverserial Model\n\n\ndef just_train():\n    \n    #low_resolution_images = input_GAN\n    #dataloader_l = iter(input_pipeline(low_resolution_images,20))\n    #high_resolution_images =np.load('High_res.npy',allow_pickle=True)\n    #dataloader_h = iter(input_pipeline(high_resolution_images,20))\n    # Build and compile VGG19 network to extract features\n    train_batches = int(np.ceil(len(low_resolution_images_train)/batch_size))-1\n    \n    for epoch in range(epochs):\n        \n        \n        for i in  range(train_batches):\n            \n            \n            \n            beg = i*batch_size\n            end = (i+1)*batch_size\n\n          \n\n                          # Generate high-resolution images from low-resolution images\n            generated_high_resolution_images = generator.predict(low_resolution_images_train[beg:end])\n            discrim_fakedata = np.concatenate((resized_low_resolution_image_train[beg:end],generated_high_resolution_images),axis=3)\n                          # Generate batch of real and fake labels\n\n                          #real_labels = np.ones((batch_size, 16, 16, 1))\n                          #fake_labels = np.zeros((batch_size, 16, 16, 1))\n\n            real_labels = np.ones((batch_size, 1))*0.95\n            fake_labels = np.ones((batch_size,  1))*0.05\n                          #print(real_labels.shape)\n\n\n\n                          # Train the discriminator network on real and fake images\n            d_loss_real = discriminator.train_on_batch(discrim_realdata[beg:end], real_labels)\n            d_loss_fake = discriminator.train_on_batch(discrim_fakedata, fake_labels)\n\n                          # Calculate total discriminator loss\n            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n                      #print(\"d_loss:\", d_loss)\n\n                  #print(high_resolution_images.shape)\n                  #high_resolution_images =np.load('High_res.npy',allow_pickle=True)\n                  #print(high_resolution_images.shape)\n\n                  #print(high_resolution_images1.shape)\n\n                  #print(high_resolution_images2.shape)\n                  # Sample a batch of images\n                  #high_resolution_images, low_resolution_images = next(highres_dataloader) , next(lowres_dataloader)\n\n\n             # \"Train the generator network\"\n\n\n                  # Sample a batch of images\n\n                  #high_resolution_images, low_resolution_images = sample_images(batch_size)\n                  #high_resolution_images, low_resolution_images = next(highres_dataloader) , next(lowres_dataloader)\n                  #high_resolution_images, low_resolution_images = next(dataloader)\n\n\n                  #high_resolution_images=np.Concatenate(high_resolution_images,high_resolution_images,high_resolution_images,axis=3)\n\n\n\n                  # Extract feature maps for real high-resolution images\n                  #rgbImage1=np.concatenate(high_resolution_images,high_resolution_images,generated_high_resolution_images)\n            image_features = vgg.predict(inverted_high_resolution_img2[beg:end])\n\n                  # Train the generator network\n            g_loss = adversarial_model.train_on_batch([low_resolution_images_train[beg:end],inverted_high_resolution_img_train[beg:end]], [real_labels,image_features])\n\n                  #print(\"g_loss:\", g_loss)\n\n        print(\"Epoch {} : g_loss: {} , d_loss: {}\".format(epoch , g_loss[0] , d_loss[0]))\n\n                  # Write the losses to Tensorboard\n                  #write_log(tensorboard, 'g_loss', g_loss[0], epoch)\n                  #write_log(tensorboard, 'd_loss', d_loss[0], epoch)\n\n                  # Sample and save images after every 100 epochs\n\n                  #with writer.as_default():\n                   ## tf.summary.scalar('g_loss', g_loss[0], step=epoch)\n                   # tf.summary.scalar('d_loss', d_loss[0], step=epoch)\n                  #writer.flush()\n\n                  #write_log(tensorboard, 'g_loss', g_loss[0], epoch)\n                  #write_log(tensorboard, 'd_loss', d_loss[0], epoch)\n\n                  #print(\"Epoch {}:  Gloss : {} , Dloss {}\".format(epoch+1 , g_loss , d_loss))\n            #if (epoch) % 100 == 0:\n                      #high_resolution_images, low_resolution_images = sample_images(batch_size)\n                      #high_resolution_images, low_resolution_images = next(highres_dataloader) , next(lowres_dataloader)\n                      #high_resolution_images, low_resolution_images = next(dataloader)\n\n                      # Normalize images\n                      #high_resolution_images = high_resolution_images / 127.5 - 1.\n                      #low_resolution_images = low_resolution_images / 127.5 - 1.\n\n            # generated_images = generator.predict_on_batch(low_resolution_images_test[beg:end])\n\n                       # for index, img in enumerate(generated_images):\n                         #      save_images(res_im_path + \"img_{}_{}\".format(epoch, index),\n                         #                 low_resolution_images[index], generated_images[index] , high_resolution_images[index]\n                         #                 )\n\n\n                #if (epoch) % 1000 == 0:\n                      # Save models\n                   #     generator.save_weights(\"generator_{}.h5\".format(epoch))\n                     #   discriminator.save_weights(\"discriminator_{}.h5\".format(epoch))\n\n              \n\n              \nif mode == 'train':\n     just_train()\n\n    \n    \n    ","metadata":{"id":"DDE-NI390sTN","outputId":"a927b0be-88fc-4135-8003-8fa339ba19e8","execution":{"iopub.status.busy":"2022-10-08T17:17:15.265421Z","iopub.execute_input":"2022-10-08T17:17:15.266125Z","iopub.status.idle":"2022-10-08T17:25:10.687446Z","shell.execute_reply.started":"2022-10-08T17:17:15.266081Z","shell.execute_reply":"2022-10-08T17:25:10.686402Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"Epoch 0 : g_loss: 414.25677490234375 , d_loss: 0.05000000633299351\nEpoch 1 : g_loss: 413.69342041015625 , d_loss: 0.05000000633299351\nEpoch 2 : g_loss: 413.893798828125 , d_loss: 0.05000000633299351\nEpoch 3 : g_loss: 413.70733642578125 , d_loss: 0.05000000633299351\nEpoch 4 : g_loss: 413.566162109375 , d_loss: 0.05000000633299351\nEpoch 5 : g_loss: 413.54205322265625 , d_loss: 0.05000000633299351\nEpoch 6 : g_loss: 413.55230712890625 , d_loss: 0.05000000633299351\nEpoch 7 : g_loss: 413.5270080566406 , d_loss: 0.05000000633299351\nEpoch 8 : g_loss: 413.4330139160156 , d_loss: 0.05000000633299351\nEpoch 9 : g_loss: 413.3768005371094 , d_loss: 0.05000000633299351\nEpoch 10 : g_loss: 413.43438720703125 , d_loss: 0.05000000633299351\nEpoch 11 : g_loss: 413.412841796875 , d_loss: 0.05000000633299351\nEpoch 12 : g_loss: 413.38140869140625 , d_loss: 0.05000000633299351\nEpoch 13 : g_loss: 413.36700439453125 , d_loss: 0.05000000633299351\nEpoch 14 : g_loss: 413.3606872558594 , d_loss: 0.05000000633299351\nEpoch 15 : g_loss: 413.34454345703125 , d_loss: 0.05000000633299351\nEpoch 16 : g_loss: 413.3285217285156 , d_loss: 0.05000000633299351\nEpoch 17 : g_loss: 413.3169860839844 , d_loss: 0.05000000633299351\nEpoch 18 : g_loss: 413.2926330566406 , d_loss: 0.05000000633299351\nEpoch 19 : g_loss: 413.29583740234375 , d_loss: 0.05000000633299351\nEpoch 20 : g_loss: 413.3074645996094 , d_loss: 0.05000000633299351\nEpoch 21 : g_loss: 413.28558349609375 , d_loss: 0.05000000633299351\nEpoch 22 : g_loss: 413.28619384765625 , d_loss: 0.05000000633299351\nEpoch 23 : g_loss: 413.27606201171875 , d_loss: 0.05000000633299351\nEpoch 24 : g_loss: 413.27520751953125 , d_loss: 0.05000000633299351\nEpoch 25 : g_loss: 413.27532958984375 , d_loss: 0.05000000633299351\nEpoch 26 : g_loss: 413.26947021484375 , d_loss: 0.05000000633299351\nEpoch 27 : g_loss: 413.2470703125 , d_loss: 0.05000000633299351\nEpoch 28 : g_loss: 413.2500915527344 , d_loss: 0.05000000633299351\nEpoch 29 : g_loss: 413.25445556640625 , d_loss: 0.05000000633299351\n","output_type":"stream"}]},{"cell_type":"code","source":"# Predict on test data and plotting interactively\n\ngenerated_images = generator.predict(low_resolution_images_test)\n\n\ndef plot_bridge4(i):\n    fig,ax = pyplot.subplots()\n    \n    #print(f\"Volume fraction: {y[i]}\")\n    im4 = pyplot.imshow(generated_images[i].reshape(128,128), cmap='gray')\n    pyplot.axis('off')\n    \n    pyplot.show()\n\ninteract(plot_bridge4, i=(0, len(generated_images)-1))\n","metadata":{"id":"gUwAK3f9_8Lm","outputId":"d7a51877-cf2f-4bc9-999a-1eb00353486e","execution":{"iopub.status.busy":"2022-10-08T17:25:12.759453Z","iopub.execute_input":"2022-10-08T17:25:12.759840Z","iopub.status.idle":"2022-10-08T17:25:13.236604Z","shell.execute_reply.started":"2022-10-08T17:25:12.759807Z","shell.execute_reply":"2022-10-08T17:25:13.235496Z"},"trusted":true},"execution_count":71,"outputs":[{"output_type":"display_data","data":{"text/plain":"interactive(children=(IntSlider(value=94, description='i', max=188), Output()), _dom_classes=('widget-interactâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d9ec6660d514a03acca45b8c73ed8c8"}},"metadata":{}},{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"<function __main__.plot_bridge4(i)>"},"metadata":{}}]},{"cell_type":"code","source":"\nimg1=Image.fromarray(generated_images[0].reshape(128,128))\nimg1.save('./GAN_1.tiff')\nimg1=Image.fromarray(generated_images[94].reshape(128,128))\nimg1.save('./GAN_95.tiff')\nimg1=Image.fromarray(generated_images[188].reshape(128,128))\nimg1.save('./GAN_189.tiff')","metadata":{"execution":{"iopub.status.busy":"2022-10-08T17:29:07.409433Z","iopub.execute_input":"2022-10-08T17:29:07.409830Z","iopub.status.idle":"2022-10-08T17:29:07.419429Z","shell.execute_reply.started":"2022-10-08T17:29:07.409797Z","shell.execute_reply":"2022-10-08T17:29:07.418513Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"# Plotting interactively Original images (ground truth) for comparison\n\n\ndef plot_bridge5(i):\n    fig,ax = pyplot.subplots()\n    \n    #print(f\"Volume fraction: {y[i]}\")\n    im1 = pyplot.imshow(inverted_high_resolution_img_test[i].reshape(128,128), cmap='gray')\n    pyplot.axis('off')\n    \n    pyplot.show()\n \n \ninteract(plot_bridge5, i=(0, len(inverted_high_resolution_img_test)-1))","metadata":{"id":"Tk_eOqcF4NeG","outputId":"533e76e7-52f5-4531-fa0b-85e9e522279d","execution":{"iopub.status.busy":"2022-10-08T17:03:07.535158Z","iopub.execute_input":"2022-10-08T17:03:07.535532Z","iopub.status.idle":"2022-10-08T17:03:07.734542Z","shell.execute_reply.started":"2022-10-08T17:03:07.535502Z","shell.execute_reply":"2022-10-08T17:03:07.733640Z"},"trusted":true},"execution_count":55,"outputs":[{"output_type":"display_data","data":{"text/plain":"interactive(children=(IntSlider(value=94, description='i', max=188), Output()), _dom_classes=('widget-interactâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3d00465628b4d5abec510804761413f"}},"metadata":{}},{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"<function __main__.plot_bridge5(i)>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nMSE_loss_fn = nn.MSELoss()\n\nhigh_resolution_img_test_new=np.expand_dims(high_resolution_images_test,axis=3)\n\nprint(generated_images.shape)\n\nprint(high_resolution_img_test_new.shape)\n\n#loss_value = MSE_loss_fn(generated_images,high_resolution_img_test_new)\ndata1 = torch.from_numpy(generated_images)\ndata2 = torch.from_numpy(high_resolution_img_test_new)\n\nLoss_value = MSE_loss_fn(data2, data1)\n\nprint(Loss_value)","metadata":{"execution":{"iopub.status.busy":"2022-10-08T14:33:07.879959Z","iopub.execute_input":"2022-10-08T14:33:07.880327Z","iopub.status.idle":"2022-10-08T14:33:08.449893Z","shell.execute_reply.started":"2022-10-08T14:33:07.880294Z","shell.execute_reply":"2022-10-08T14:33:08.447358Z"},"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_70453/3355994198.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mMSE_loss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mhigh_resolution_img_test_new\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhigh_resolution_images_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"],"ename":"NameError","evalue":"name 'np' is not defined","output_type":"error"}]},{"cell_type":"code","source":"import cv2\nimport image_similarity_measures\nfrom image_similarity_measures.quality_mtrics import rmse, psnr\nimage-similarity-measures --org_img_path=path_to_first_img\n--pred_img_path=path_to_second_img --mode=tif\nout_rmse = rmse(high_resolution_img_test_new[0], generated_images[0])\nout_psnr = psnr(high_resolution_img_test_new[0], generated_images[0])","metadata":{"execution":{"iopub.status.busy":"2022-10-06T19:28:42.234731Z","iopub.execute_input":"2022-10-06T19:28:42.235702Z","iopub.status.idle":"2022-10-06T19:28:42.243833Z","shell.execute_reply.started":"2022-10-06T19:28:42.235642Z","shell.execute_reply":"2022-10-06T19:28:42.242356Z"},"trusted":true},"execution_count":149,"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_32558/1352205111.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    image-similarity-measures --org_img_path=path_to_first_img\u001b[0m\n\u001b[0m                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't assign to operator\n"],"ename":"SyntaxError","evalue":"can't assign to operator (1352205111.py, line 4)","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}